apiVersion: v1
kind: PersistentVolume
metadata:
  name: host-pv
spec:
  # host path persistent volume , but now detached from pod
  # but it won't be independent of node
  hostPath:
    path: /data
    type: DirectoryOrCreate
  capacity:
    # matters, where we can decide how much size pods can use
    # deployed in our cluster
    storage: 1Gi
  # either file or block, generally available storages
  volumeMode: Filesystem
  # ReadWriteOnce -> can be mounted as read write volume by multiple pods on single node, so pods should belong with single node
  # ReadOnlyMany -> can be mounted as read volume by multiple pods on multiple node, so multiple pods  on multiple nodes  can claim these (not available for host path)
  # ReadWriteMany -> can be mounted as read write volume by multiple pods on multiple node, so multiple pods  on multiple nodes  can claim these (not available for host path)
  accessModes:
  - ReadWriteOnce
  # how the storage should be provisioned
  # kubectl get sc
  # kinD already configured this for us
  storageClassName: standard

---
# this should be then used in all pods which required persistent volume
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: host-pvc
spec:
  accessModes:
  - ReadWriteOnce
  storageClassName: standard
  # requesting storage of 1Gi
  resources:
    requests:
      storage: 1Gi
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: data-store-env
# the data to be passed as evn variables
data:
  # key: value
  # can have lot manys
  folder: 'userchanges_1'
  port: '3000'
---
# what service should have - the deployment which it want to expose, the port of pod and  port at which
# application is running
# pl refer https://kubernetes.io/docs/reference/kubernetes-api/service-resources/service-v1/

apiVersion: v1
# k8 artifacts
kind: Service
# its own name
metadata:
  name: backend
# deployment specs
spec:
  # select pods with lable app: second-app (pods may have multiple labels)
  # the pods in our case is having second label tier: backend
  # the pods can also be placed say with label tier: another-backend
  # so selecting just app: second-app is targetting all the tiers in which this app can be placed
  selector:
    app: second-app
  # ports specs
  ports:
  # IP protocol
  - protocol: "TCP"
    # port that will be exposed by this service
    port: 80
    # port number to access on the pods targeted by the service (mapped to application)
    targetPort: 3000
    # to allow traffic inside kinD cluster
    nodePort: 30001
  # type of service
  type: NodePort
---
# The deployment object first should have its own name , then the container and pod containing that container
# and then selecting this pod as a part of deployment

# refer https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/deployment-v1/
# will list all details of enclosing specs
apiVersion: apps/v1
# Deployment is k8 artifact or k8 object
kind: Deployment
metadata:
  # name of deployment
  name: second-app-deployment
# deployment specs, how the deployment should be configured
spec:
  # how many pods, default is 1, even 0 can be given
  replicas: 1
  # pods specs, which deployment should have
  template:
    # pods should be labelled with
    # there is no need for kind: Pod  here as deployment will always have pod
    metadata:
      # pod should be labelled with
      labels:
        app: second-app
        tier: backend
    # how this pod should look like
    spec:
      # how containers should be created inside pods
      # for this deployment, how many replicas you create , all pods will have same container spec
      containers:
      # container name inside pod and image
      - name: second-app-container
        # you need to create this image and push to registry
        image: cbagade/kube-data-icvolcm:v5
        env:
        # process.env.USERCHANGES_FOLDER
        - name: USERCHANGES_FOLDER
          # value: 'userchanges'
          # now access from config map
          valueFrom:
            configMapKeyRef:
              name: data-store-env
              key: folder
              # process.env.port in app.js
        - name: PORT
          # value: 'userchanges'
          # now access from config map
          valueFrom:
            configMapKeyRef:
              name: data-store-env
              key: port
        # after setting vol at pod, we need to mount that on container on specific path
        # this application is already have story folder, which will be inside app folder (see Dockerfile)
        # so that text file will be available onto pod now, over volume
        volumeMounts:
        - mountPath: /app/userchanges
          # name of volume set on pod, there might be multiple volumes, so need names
          name: userchanges-volume
      # volumes are at container level on pods
      volumes:
      # following volumes can be used by containers then
      # this should be defined first and then the volumeMounts should be added to containers
      # 
      - name: userchanges-volume
        # we are now putting claim on pods to request PV
        persistentVolumeClaim:
          claimName: host-pvc
  # what pods should be created as a part of deployment
  # see template section, where pods are labelled
  # those pods will be selected to form this deployment
  selector:
    # the labels will be matched based on labels of pods
    matchLabels:
      app: second-app
      tier: backend



# since the host path is used, can do docker exec -it <kind_node> /bin/bash and clear /data directory on node
# kubectl apply -f app_deployment.yaml
# kubectl get pv --> status is Bound
# kubectl get pvc --> status is Bound

# GET app_2/api/v1/fruits change port to 30001
# POST  app_2/api/v1/fruits change port to 30001
# Verify the changes
# GET  app_2/api/v1/fruits change port to 30001
# GET  app_2/api/v1/fruits/error change port to 30001
# This action will put container in error state and restart (pod is not restarted)
# GET app_2/api/v1/fruits change port to 30001
# changes are available, so hostPath can survice container restart
# kubectl delete -f app_deployment.yaml
# kubectl apply -f app_deployment.yaml
# GET app_2/api/v1/fruits change port to 30001
# changes are lost, so hostPath can survive pod restart


